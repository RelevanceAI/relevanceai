{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8Zd9EhXTqerj",
      "metadata": {
        "id": "8Zd9EhXTqerj"
      },
      "source": [
        "# Quickstart\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RelevanceAI/RelevanceAI-readme-docs/blob/v2.0.0/docs/getting-started/example-applications/_notebooks/RelevanceAI-ReadMe-Text-to-Image-Search.ipynb)\n",
        "\n",
        "\n",
        "[Try the image search live in Relevance AI Dashboard](https://cloud.relevance.ai/demo/search/image-to-text).\n",
        "\n",
        "\n",
        "In this notebook we will show you how to create and experiment with a powerful text to image search engine using OpenAI's CLIP and Relevance AI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928fd38d",
      "metadata": {
        "id": "928fd38d"
      },
      "source": [
        "# What I Need"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31dfada7",
      "metadata": {
        "id": "31dfada7"
      },
      "source": [
        "- Project & API Key (The SDK will link you to the corresponding page or you can grab your API key from https://cloud.relevance.ai/ in the settings area)\n",
        "- Python 3\n",
        "- Relevance AI Installed as shown below. For more information visit [Installation guide](https://docs.relevance.ai/docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VC8aPkNVQagh",
      "metadata": {
        "id": "VC8aPkNVQagh"
      },
      "source": [
        "## Installation Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OOJCjWZkQb15",
      "metadata": {
        "id": "OOJCjWZkQb15"
      },
      "outputs": [],
      "source": [
        "# Relevance AI installation\n",
        "# remove `!` if running the line in a terminal\n",
        "!pip install -U RelevanceAI[notebook]==2.0.0\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dMV_xgPVVjtZ",
      "metadata": {
        "id": "dMV_xgPVVjtZ"
      },
      "source": [
        "## Client Setup\n",
        "\n",
        "You can sign up/login and find your credentials here: https://cloud.relevance.ai/sdk/api\n",
        "Once you have signed up, click on the value under `Activation token` and paste it here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qsicetXLViu8",
      "metadata": {
        "id": "qsicetXLViu8"
      },
      "outputs": [],
      "source": [
        "from relevanceai import Client\n",
        "client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Si85Fxvtq4W5",
      "metadata": {
        "id": "Si85Fxvtq4W5"
      },
      "source": [
        "# Text-to-image search\n",
        "\n",
        "To enable text-to-image search we will be using Relevance AI as the vector database and OpenAI's CLIP as the vectorizer, to vectorize text and images into CLIP vector embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "keeAanisPnYx",
      "metadata": {
        "id": "keeAanisPnYx"
      },
      "source": [
        "## 1) Data\n",
        "For this quickstart we will be using a sample e-commerce dataset. Alternatively, you can use your own dataset for the different steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I1NqdL82piq6",
      "metadata": {
        "id": "I1NqdL82piq6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from relevanceai.utils.datasets import get_ecommerce_dataset_clean\n",
        "\n",
        "# Retrieve our sample dataset. - This comes in the form of a list of documents.\n",
        "documents = get_ecommerce_dataset_clean()\n",
        "pd.DataFrame.from_dict(documents).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ETgfqlEx00E",
      "metadata": {
        "id": "3ETgfqlEx00E"
      },
      "source": [
        "## 2) Encode / Vectorize with CLIP\n",
        "CLIP is a vectorizer from OpenAI that is trained to find similarities between text and image pairs. In the code below we set up CLIP. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0r5P1MwdPnYx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r5P1MwdPnYx",
        "outputId": "46d813b5-1dca-4407-8df0-f9300a8a94f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 52.0MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import clip\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# First - let's encode the image based on CLIP\n",
        "def encode_image(image):\n",
        "    # Let us download the image and then preprocess it\n",
        "    image = preprocess(Image.open(requests.get(image, stream=True).raw)).unsqueeze(0).to(device)\n",
        "    # We then feed our processed image through the neural net to get a vector\n",
        "    with torch.no_grad():\n",
        "      image_features = model.encode_image(image)\n",
        "    # Lastly we convert it to a list so that we can send it through the SDK\n",
        "    return image_features.tolist()[0]\n",
        "\n",
        "# Next - let's encode text based on CLIP\n",
        "def encode_text(text):\n",
        "    # let us get text and then tokenize it\n",
        "    text = clip.tokenize([text]).to(device)\n",
        "    # We then feed our processed text through the neural net to get a vector\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text)\n",
        "    return text_features.tolist()[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eSGvSdtax8bO",
      "metadata": {
        "id": "eSGvSdtax8bO"
      },
      "source": [
        "We then encode the data we have into vectors, this will take a couple of mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z80cq1Wf3TSP",
      "metadata": {
        "id": "Z80cq1Wf3TSP"
      },
      "outputs": [],
      "source": [
        "documents = documents[:500] # only 500 docs to make the process faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dmZNIU0x5L6",
      "metadata": {
        "id": "2dmZNIU0x5L6"
      },
      "outputs": [],
      "source": [
        "def encode_image_document(d):\n",
        "  try:\n",
        "    d['product_image_clip_vector_'] = encode_image(d['product_image'])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# Let's import TQDM for a nice progress bar!\n",
        "from tqdm.auto import tqdm\n",
        "[encode_image_document(d) for d in tqdm(documents)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PfEdja6Hp3uy",
      "metadata": {
        "id": "PfEdja6Hp3uy"
      },
      "source": [
        "## 3) Insert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r-2qgvOLyPTi",
      "metadata": {
        "id": "r-2qgvOLyPTi"
      },
      "source": [
        "Uploading our documents into the dataset `quickstart_clip`.\n",
        "\n",
        "In case you are uploading your own dataset, keep in mind that each document should have a field called '_id'. Such an id can be easily allocated using the uuid package:\n",
        "\n",
        "```\n",
        "ds.insert_documents(documents, create_id=True)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZAVHLeiNp55k",
      "metadata": {
        "id": "ZAVHLeiNp55k"
      },
      "outputs": [],
      "source": [
        "ds = client.Dataset(\"quickstart_clip\")\n",
        "ds.insert_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TLUEkCW_THXF",
      "metadata": {
        "id": "TLUEkCW_THXF"
      },
      "source": [
        "Once we have uploaded the data, we can see the dataset on the [dashboard](https://cloud.relevance.ai/dataset/quickstart_clip/dashboard/monitor/vectors). \n",
        "\n",
        "The dashboard provides users with a great overview and statistics of the dataset as shown below. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oLJKZwd1p8Wf",
      "metadata": {
        "id": "oLJKZwd1p8Wf"
      },
      "source": [
        "## 4) Search\n",
        "This step is to run a simple vector search; you can read more about vector search and how to construct a multi-vector query [here](https://docs.relevance.ai/docs/hybrid-search). \n",
        "\n",
        "Note that our dataset includes vectors generated by the Clip encoder. Therefore, in this step, we first vectorize the query using the same encoder to be able to search among the similarly generated vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ahFxCSbGPnYy",
      "metadata": {
        "id": "ahFxCSbGPnYy"
      },
      "outputs": [],
      "source": [
        "\n",
        "query = 'for my baby daughter'\n",
        "query_vector = encode_text(query)\n",
        "multivector_query=[\n",
        "    { \"vector\": query_vector, \"fields\": [\"product_image_clip_vector_\"]}\n",
        "]\n",
        "results = ds.vector_search(\n",
        "    multivector_query=multivector_query,\n",
        "    page_size=5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yQ1zWs5WdWuS",
      "metadata": {
        "id": "yQ1zWs5WdWuS"
      },
      "source": [
        "You can use our json shower library to observe the search result in a notebook as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lmL7cE3vPnYy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "lmL7cE3vPnYy",
        "outputId": "81871cbb-2ef4-4f09-c28b-07a9635f9902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== QUERY ===>   for my baby daughter\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_image</th>\n",
              "      <th>product_title</th>\n",
              "      <th>_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td><img src=\"https://ak1.ostkcdn.com/images/products/9392460/P16581616.jpg\" width=\"60\" ></td>\n",
              "      <td>Crocs Girl (Infant) 'Littles Hover' Leather Athletic Shoe</td>\n",
              "      <td>cdf48ecc-882a-45ab-b625-ba86bf8cffa4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td><img src=\"https://ak1.ostkcdn.com/images/products/9669945/P16850773.jpg\" width=\"60\" ></td>\n",
              "      <td>The New York Doll Collection Double Stroller</td>\n",
              "      <td>ae2915f9-d7bb-4e0c-8a05-65682cd5a6d3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td><img src=\"https://ak1.ostkcdn.com/images/products/5158127/Badger-Basket-Envee-Baby-High-Chair-Play-Table-in-Pink-P12999228.jpg\" width=\"60\" ></td>\n",
              "      <td>Badger Basket Envee Baby High Chair/ Play Table in Pink</td>\n",
              "      <td>585e7877-95eb-4864-9d89-03d5369c08fa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td><img src=\"https://ak1.ostkcdn.com/images/products/9151116/P16330850.jpg\" width=\"60\" ></td>\n",
              "      <td>Crocs Girl (Toddler) 'CC Magical Day Princess' Synthetic Casual Shoes (Size 6 )</td>\n",
              "      <td>14c3ad94-3ecd-438b-b00e-1ce5b0eed4e3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td><img src=\"https://ak1.ostkcdn.com/images/products/9151116/P16330850.jpg\" width=\"60\" ></td>\n",
              "      <td>Crocs Girl (Toddler) 'CC Magical Day Princess' Synthetic Casual Shoes (Size 6 )</td>\n",
              "      <td>30809211-dbcd-4b15-8c0a-7702dfe9e30f</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from relevanceai import show_json\n",
        "\n",
        "print('=== QUERY === ')\n",
        "print(query)\n",
        "\n",
        "print('=== RESULTS ===')\n",
        "show_json(results, image_fields=[\"product_image\"], text_fields=[\"product_title\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E_Lp28rM5X4G",
      "metadata": {
        "id": "E_Lp28rM5X4G"
      },
      "source": [
        "Other Notebooks:\n",
        "\n",
        "- [Multivector search with your own vectors](doc:search-with-your-own-vectors) \n",
        "- [Text search using USE (VectorHub)](doc:quickstart-text-search) \n",
        "- [Question answering using USE QA (Tensorflow Hub)](doc:quickstart-question-answering) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8Zd9EhXTqerj"
      ],
      "name": "RelevanceAI-ReadMe-Text-to-Image-Search.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "384px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
